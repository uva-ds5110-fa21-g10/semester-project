{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Stage 1\n",
    "\n",
    "For this notebook, we load our source chess game data from lichess, applying a schema in the process, and filter the games down to only those following the rules of Classic and Blitz.\n",
    "From there, we write our filtered dataset out to a parquet file for future notebook processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"fa21-ds5110-group10\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Lengith 6256184\n",
      "Filtered Dataset Lengith 3850385\n"
     ]
    }
   ],
   "source": [
    "chess_schema = StructType([StructField('event', StringType(), False), \n",
    "                           StructField('white', StringType(), False),\n",
    "                           StructField('black', StringType(), False),\n",
    "                           StructField('result', StringType(), False),\n",
    "                           StructField('UTCDate', DateType(), False),\n",
    "                           StructField('UTCTime', StringType(), False),\n",
    "                           StructField('WhiteElo', IntegerType(), False),\n",
    "                           StructField('BlackElo', IntegerType(), False),\n",
    "                           StructField('WhiteRatingDiff', DoubleType(), False),\n",
    "                           StructField('BlackRatingDiff', DoubleType(), False),\n",
    "                           StructField('ECO', StringType(), False),\n",
    "                           StructField('Opening', StringType(), False),\n",
    "                           StructField('TimeControl', StringType(), False),\n",
    "                           StructField('Termination', StringType(), False),\n",
    "                           StructField('AN', StringType(), False)])\n",
    "\n",
    "\n",
    "df = spark.read.csv(path=\"../../data/raw/chess_games.csv\",\n",
    "                    schema=chess_schema,\n",
    "                    header=True,\n",
    "                    ignoreLeadingWhiteSpace=True,\n",
    "                    ignoreTrailingWhiteSpace=True,\n",
    "                    dateFormat='yyyy.mm.dd')\n",
    "\n",
    "print(f'Original Dataset Length: {df.count()}')\n",
    "\n",
    "df = df.filter((F.col('event') == 'Classical') | (F.col('event') == 'Blitz'))\n",
    "print(f'Filtered Dataset Length: {df.count()}')\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"../../data/processed/chess_games_blitz_classic.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
