{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the final model, the metric like F1 and accuracy are pulled out for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fractions import Fraction as frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"fa21-ds5110-group10-rk\") \\\n",
    "    .config(\"spark.driver.memory\", \"36g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://udc-ba34-25c9:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fa21-ds5110-group10-rk</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=fa21-ds5110-group10-rk>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add in our cached dataset from our prior feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"../../../data/processed/chess_games_moves_model.parquet\")\n",
    "trainData = spark.read.parquet(\"../../../data/processed/training.parquet\")\n",
    "testData = spark.read.parquet(\"../../../data/processed/testing.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event: string, white_result: string, first_two: array<string>, ECO: string, EloDiff: int, Opening: string, game_complexity: int, opening_class: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------------+---+-------+--------------------+---------------+--------------+\n",
      "|event|white_result|       first_two|ECO|EloDiff|             Opening|game_complexity| opening_class|\n",
      "+-----+------------+----------------+---+-------+--------------------+---------------+--------------+\n",
      "|Blitz|         win|[c4 c5, Nc3 Nf6]|A34|    222|English Opening: ...|              6|Flank openings|\n",
      "|Blitz|         win| [d4 b6, c4 Bb7]|A40|    309|  English Defense #2|              5|Flank openings|\n",
      "+-----+------------+----------------+---+-------+--------------------+---------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossValidatorModel.load(\"ds5110/project/semester-project/data/modeling/GBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we review the resulting data points of interest.\n",
    "We notice that ECO and the first two sets of moves are distinct of one another, and may influence the overall model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_dd791a802d6d"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='BinaryClassificationEvaluator_811850a15b75', name='metricName', doc='metric name in evaluation (areaUnderROC|areaUnderPR)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getEvaluator().metricName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin to build a model, keying in on the opening move and the white_result columns.\n",
    "Note that both of these are categorical values, so we will need to encode them using the StringIndexer for pyspark to do model evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "opening_vectorizor = StringIndexer(inputCol=\"ECO\", outputCol=\"opening_ohe\")\n",
    "gametype_vectorizer = StringIndexer(inputCol=\"event\", outputCol=\"event_vector\")\n",
    "class_vectorizer = StringIndexer(inputCol=\"opening_class\", outputCol=\"opening_class_vector\")\n",
    "# target\n",
    "result_vectorizor = StringIndexer(inputCol=\"white_result\", outputCol=\"white_result_vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform One-Hot Encoding on our Opening type (or ECO) and do our comparision.  THis will create a new column that we will use for our random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_encoder = OneHotEncoder(inputCols=[\"opening_ohe\"],\n",
    "                        outputCols=[\"ECO_Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoder = OneHotEncoder(inputCols=[\"opening_class_vector\"],\n",
    "                        outputCols=[\"Class_Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the OHE of our ECO, we can combine it with other features to build out our predictors for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_assembler = VectorAssembler(inputCols=['ECO_Type', \"Class_Type\",\"EloDiff\"], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is model-ready, we will do a split, fit, transform, and evaluation to determine the performance of our model.\n",
    "Note that we have chosen the default tunings, but in the future we will likely apply a cross-validation technique in pyspark to select the correct hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(maxDepth=8, maxIter=25, labelCol='white_result_vector', seed=1337, leafCol=\"leafId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = Pipeline(stages=[opening_vectorizor,\n",
    "                            gametype_vectorizer,\n",
    "                            class_vectorizer,\n",
    "                            result_vectorizor,\n",
    "                            move_encoder,\n",
    "                            class_encoder,\n",
    "                            features_assembler])\n",
    "ml_pipeline = Pipeline(stages=[gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = data_pipeline.fit(df)\n",
    "data_model_train = data_model.transform(trainData)\n",
    "data_model_test = data_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossValidation set up\n",
    "stepSize is aka Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(data_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------------+---+-------+-------------+---------------+--------------+-----------+------------+--------------------+-------------------+---------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|event|white_result|       first_two|ECO|EloDiff|      Opening|game_complexity| opening_class|opening_ohe|event_vector|opening_class_vector|white_result_vector|       ECO_Type|   Class_Type|            features|       rawPrediction|         probability|prediction|              leafId|\n",
      "+-----+------------+----------------+---+-------+-------------+---------------+--------------+-----------+------------+--------------------+-------------------+---------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|Blitz|        loss| [Na3 d5, c4 c6]|A00|   -392|Sodium Attack|              3|Flank openings|        0.0|         0.0|                 2.0|                0.0|(492,[0],[1.0])|(4,[2],[1.0])|(497,[0,494,496],...|[0.88227884945948...|[0.85377956136567...|       0.0|[0.0,1.0,0.0,3.0,...|\n",
      "|Blitz|        loss|[Na3 e5, Nh3 d5]|A00|    116|Sodium Attack|              7|Flank openings|        0.0|         0.0|                 2.0|                0.0|(492,[0],[1.0])|(4,[2],[1.0])|(497,[0,494,496],...|[-0.2524259924309...|[0.37640111367006...|       1.0|[98.0,35.0,45.0,3...|\n",
      "+-----+------------+----------------+---+-------+-------------+---------------+--------------+-----------+------------+--------------------+-------------------+---------------+-------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='white_result_vector')\n",
    "model_auc = auc_evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68.79% (minInstancesPerNode = 25, maxDepth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multi = MulticlassClassificationEvaluator(labelCol='white_result_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6308771911614864"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_multi.evaluate(result, {eval_multi.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307975287351031"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_multi.evaluate(result, {eval_multi.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6449819685713402"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_multi.evaluate(result, {eval_multi.metricName: \"recallByLabel\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6301648693993191"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_multi.evaluate(result, {eval_multi.metricName: \"precisionByLabel\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6879095070531936\n"
     ]
    }
   ],
   "source": [
    "print('AUC:', model_auc)\n",
    "#print('AUC:', BinaryClassificationMetrics(gbpredictions['label','prediction'].rdd).areaUnderROC)\n",
    "#print('PR:', BinaryClassificationMetrics(gbpredictions['label','prediction'].rdd).areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rdd = MulticlassMetrics(result.select(['prediction','white_result_vector']).rdd.map(tuple))\n",
    "confusionmatrix = pd.DataFrame(metrics_rdd.confusionMatrix().toArray())\n",
    "\n",
    "label = 1.0\n",
    "best_metric = pd.DataFrame([    ['Accuracy',  metrics_rdd.accuracy],\n",
    "                                ['Precision', metrics_rdd.precision(label)],\n",
    "                                ['Recall',    metrics_rdd.recall(label)],\n",
    "                                ['F1 Score',  metrics_rdd.fMeasure(label)]],\n",
    "                              columns=['Metric', 'Measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375226.0</td>\n",
       "      <td>206536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220215.0</td>\n",
       "      <td>354145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  375226.0  206536.0\n",
       "1  220215.0  354145.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.630877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.631634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.616591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.624022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric   Measure\n",
       "0   Accuracy  0.630877\n",
       "1  Precision  0.631634\n",
       "2     Recall  0.616591\n",
       "3   F1 Score  0.624022"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6882407011502037,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 2}),\n",
       " (0.688292526238363,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 8,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 2}),\n",
       " (0.6882437569887513,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 10}),\n",
       " (0.6883048537003159,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 8,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 10}),\n",
       " (0.6882463689948984,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 25}),\n",
       " (0.6883299132049571,\n",
       "  {Param(parent='GBTClassifier_b3b65340241d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 8,\n",
       "   Param(parent='GBTClassifier_b3b65340241d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 25})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model.avgMetrics, model.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our model's performance seems to work well, with roughtly 50% of games resulting in a match to white win.\n",
    "\n",
    "Let's review our classifications and confusion matrix next to determine the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#designer_matrix = rf_result.select(['prediction','white_result_vector'])\n",
    "#metrics_rdd = MulticlassMetrics(designer_matrix.rdd.map(tuple))\n",
    "#print(metrics_rdd.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing this model, we see that this model is not good.\n",
    "Of the three classes, we found that the precision for the white loss performs okay, however the white win and tie both evaluate to 0 for precision.\n",
    "This means that our model is overfitting and failed to correctly identify either ties or losses.\n",
    "\n",
    "Further tuning will be required to better distribute our data (either adjusting the threshold, or tuning the tree)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
