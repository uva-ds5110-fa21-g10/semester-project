{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporatory Analytics of Dataset\n",
    "\n",
    "## DS 5110\n",
    "* Fall 2021\n",
    "* October 3rd\n",
    "* Group 10\n",
    "  * Antone Edelman\n",
    "  * Xin Huang\n",
    "  * Robert Knuuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"fa21-ds5110-group10-rk\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://udc-ba33-13c7:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fa21-ds5110-group10-rk</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=fa21-ds5110-group10-rk>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"../../data/processed/chess_games_blitz_classic.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some additional cleanup we have to do beyond the filtering of games to just classic and blitz.\n",
    "Below are our transforms to add new features and to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- white: string (nullable = true)\n",
      " |-- black: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- UTCDate: date (nullable = true)\n",
      " |-- UTCTime: string (nullable = true)\n",
      " |-- WhiteElo: integer (nullable = true)\n",
      " |-- BlackElo: integer (nullable = true)\n",
      " |-- WhiteRatingDiff: double (nullable = true)\n",
      " |-- BlackRatingDiff: double (nullable = true)\n",
      " |-- ECO: string (nullable = true)\n",
      " |-- Opening: string (nullable = true)\n",
      " |-- TimeControl: string (nullable = true)\n",
      " |-- Termination: string (nullable = true)\n",
      " |-- AN: string (nullable = true)\n",
      "\n",
      "Abandoned Games to drop: 739\n",
      "Refined schema\n",
      "-------------------\n",
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- white: string (nullable = true)\n",
      " |-- black: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- UTCDate: date (nullable = true)\n",
      " |-- UTCTime: string (nullable = true)\n",
      " |-- WhiteElo: integer (nullable = true)\n",
      " |-- BlackElo: integer (nullable = true)\n",
      " |-- WhiteRatingDiff: double (nullable = true)\n",
      " |-- BlackRatingDiff: double (nullable = true)\n",
      " |-- ECO: string (nullable = true)\n",
      " |-- Opening: string (nullable = true)\n",
      " |-- TimeControl: string (nullable = true)\n",
      " |-- Termination: string (nullable = true)\n",
      " |-- AN: string (nullable = true)\n",
      " |-- moves: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- white_games_won: double (nullable = true)\n",
      " |-- black_games_won: double (nullable = true)\n",
      " |-- tie: boolean (nullable = true)\n",
      " |-- result_moves: integer (nullable = false)\n",
      " |-- game_complexity: integer (nullable = false)\n",
      " |-- EloDiff: integer (nullable = true)\n",
      " |-- first_ten: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- first_two: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- white_result: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "print(\"Abandoned Games to drop: {}\".format(df.filter(F.col(\"result\").contains(\"*\")).count()))\n",
    "# Remove all abandoned games.\n",
    "df_filtered = df.filter(~F.col(\"result\").contains(\"*\"))\n",
    "\n",
    "from fractions import Fraction as frac\n",
    "\n",
    "# We break apar the AN notation for chess moves into arrays\n",
    "def movetype(x):\n",
    "    import re\n",
    "    moves = re.split('\\d+\\. ', x)[1:]\n",
    "    return [x.strip() for x in moves]\n",
    "\n",
    "\n",
    "udf_movetype = F.udf(lambda x: movetype(x), T.ArrayType(T.StringType()))\n",
    "df_filtered = df_filtered.withColumn('moves', udf_movetype(F.col('AN')))\n",
    "\n",
    "# Convert result column into seperate white/black win columns\n",
    "white_win_udf = F.udf(lambda result: float(frac(result.split('-')[0])), T.DoubleType())\n",
    "df_filtered = df_filtered.withColumn(\"white_games_won\", white_win_udf(F.col(\"result\")))\n",
    "black_win_udf = F.udf(lambda result: float(frac(result.split('-')[1])), T.DoubleType())\n",
    "df_filtered = df_filtered.withColumn(\"black_games_won\", black_win_udf(F.col(\"result\")))\n",
    "df_filtered = df_filtered.withColumn(\"tie\", F.col(\"white_games_won\") == F.col(\"black_games_won\"))\n",
    "\n",
    "# Identify the total number of moves in a game\n",
    "df_filtered = df_filtered.withColumn(\"result_moves\", F.size(F.col(\"moves\")))\n",
    "# Categorize games based upon total move size.\n",
    "df_filtered = df_filtered.withColumn(\"game_complexity\",\n",
    "                                     F.when(F.col(\"result_moves\") ==  1, 1)\\\n",
    "                                      .when(F.col(\"result_moves\") <= 10, 2)\\\n",
    "                                      .when(F.col(\"result_moves\") <= 20, 3)\\\n",
    "                                      .when(F.col(\"result_moves\") <= 30, 4)\\\n",
    "                                      .when(F.col(\"result_moves\") <= 40, 5)\\\n",
    "                                      .when(F.col(\"result_moves\") <= 50, 6)\\\n",
    "                                      .otherwise(7))\n",
    "\n",
    "df_filtered = df_filtered.withColumn(\"EloDiff\", F.col(\"WhiteElo\") - F.col(\"BlackElo\"))\n",
    "\n",
    "\n",
    "# Collect only the first subset of moves in a game\n",
    "df_filtered = df_filtered.withColumn(\"first_ten\", F.slice(F.col(\"moves\"), 1, 10))\n",
    "df_filtered = df_filtered.withColumn(\"first_two\", F.slice(F.col(\"moves\"), 1, 2))\n",
    "\n",
    "# We establish a formal win column that we'll be using as a logistical response\n",
    "# We will not consider a tie as a win for white.\n",
    "df_filtered = df_filtered.withColumn(\"white_result\",\n",
    "                                     F.when(F.col(\"white_games_won\") > 0.5, \"win\")\\\n",
    "                                      .otherwise(\"loss\"))\n",
    "\n",
    "print(\"Refined schema\\n-------------------\")\n",
    "df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------------+---+-------+--------------------+---------------+\n",
      "|    event|white_result|       first_two|ECO|EloDiff|             Opening|game_complexity|\n",
      "+---------+------------+----------------+---+-------+--------------------+---------------+\n",
      "|    Blitz|         win|[c4 c5, Nc3 Nf6]|A34|    222|English Opening: ...|              6|\n",
      "|    Blitz|         win| [d4 b6, c4 Bb7]|A40|    309|  English Defense #2|              5|\n",
      "|Classical|        loss| [e4 Nc6, d4 d5]|B00|   -248|Nimzowitsch Defen...|              7|\n",
      "|    Blitz|         win|[e4 e5, d4 exd4]|C21|   -212|       Danish Gambit|              4|\n",
      "|    Blitz|        loss|[e4 e5, d4 exd4]|C22|     75|Center Game: Paul...|              4|\n",
      "+---------+------------+----------------+---+-------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vars_to_keep = [\"event\", \n",
    "                \"white_result\", \n",
    "#              \"WhiteElo\", \n",
    "#              \"BlackElo\", \n",
    "#              \"first_ten\",\n",
    "                \"first_two\",\n",
    "                \"ECO\",\n",
    "                \"EloDiff\",\n",
    "                \"Opening\",\n",
    "                \"game_complexity\"]\n",
    "\n",
    "df_filtered.select(vars_to_keep).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the dataframe on these predictors\n",
    "df_final = df_filtered.select(vars_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.parquet(\"../../data/processed/chess_games_moves_model.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data for future modeling using a 60, 30, 10 split for training, testing, and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, validationData  = df_final.randomSplit([0.6, 0.3, 0.1], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.write.parquet(\"../../data/processed/training.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.write.parquet(\"../../data/processed/testing.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationData.write.parquet(\"../../data/processed/validation.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
