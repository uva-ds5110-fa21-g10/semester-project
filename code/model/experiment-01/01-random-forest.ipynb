{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"fa21-ds5110-group10\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"../../data/processed/chess_games_blitz_classic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+------+----------+--------+--------+--------+---------------+---------------+---+--------------------+-----------+------------+--------------------+\n",
      "|    event|         white|      black|result|   UTCDate| UTCTime|WhiteElo|BlackElo|WhiteRatingDiff|BlackRatingDiff|ECO|             Opening|TimeControl| Termination|                  AN|\n",
      "+---------+--------------+-----------+------+----------+--------+--------+--------+---------------+---------------+---+--------------------+-----------+------------+--------------------+\n",
      "|    Blitz|        Nippis|   Misha_44|   1-0|2016-01-26|18:03:38|    2068|    1846|           11.0|           -5.0|A34|English Opening: ...|      300+0|      Normal|1. c4 c5 2. Nc3 N...|\n",
      "|    Blitz|   abracadaver| andremoniy|   1-0|2016-01-26|18:03:39|    1708|    1399|            3.0|           -3.0|A40|  English Defense #2|      180+0|      Normal|1. d4 b6 2. c4 Bb...|\n",
      "|Classical|  tewarisachin|mohamad9003|   0-1|2016-01-26|18:03:39|    1542|    1790|           -6.0|            5.0|B00|Nimzowitsch Defen...|      600+0|Time forfeit|1. e4 Nc6 2. d4 d...|\n",
      "|    Blitz|       shamshi|  kbsanswer|   1-0|2016-01-26|18:03:42|    1467|    1679|           18.0|          -17.0|C21|       Danish Gambit|      180+1|      Normal|1. e4 e5 2. d4 ex...|\n",
      "|    Blitz|yourkingismine|   BurneyXM|   0-1|2016-01-26|18:03:40|    1249|    1174|          -15.0|           14.0|C22|Center Game: Paul...|      300+0|      Normal|1. e4 e5 2. d4 ex...|\n",
      "+---------+--------------+-----------+------+----------+--------+--------+--------+---------------+---------------+---+--------------------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event='Blitz', white='Nippis', black='Misha_44', result='1-0', UTCDate=datetime.date(2016, 1, 26), UTCTime='18:03:38', WhiteElo=2068, BlackElo=1846, WhiteRatingDiff=11.0, BlackRatingDiff=-5.0, ECO='A34', Opening='English Opening: Symmetrical Variation, Normal Variation', TimeControl='300+0', Termination='Normal', AN='1. c4 c5 2. Nc3 Nf6 3. Nf3 g6 4. d4 cxd4 5. Nxd4 Bg7 6. g3 O-O 7. Bg2 Nc6 8. O-O Nxd4 9. Qxd4 d6 10. Qd2 Rb8 11. Rd1 Qa5 12. a3 a6 13. b4 Qd8 14. Bb2 Be6 15. Nd5 Nxd5 16. cxd5 Bd7 17. Rac1 Rc8 18. Bxg7 Kxg7 19. Qd4+ f6 20. Qa7 Rb8 21. Rc3 Rf7 22. Rdc1 Bb5 23. e4 h5 24. Bh3 h4 25. Be6 Rf8 26. Rc7 hxg3 27. hxg3 f5 28. Rxb7 Rxb7 29. Qxb7 fxe4 30. Rc7 Kf6 31. Rc8 e3 32. Rxd8 Rxd8 33. fxe3 Rh8 34. Qb6 Rh5 35. Qd4+ Re5 36. Qf4+ Rf5 37. Bxf5 gxf5 38. e4 Bd3 39. Qxf5+ Kg7 40. g4 Kg8 41. g5 Kg7 42. g6 a5 43. Qf7+ Kh6 44. g7 Bxe4 45. g8=Q Bxd5 46. Qfh7# 1-0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broke strings into arrary\n",
    "def movetype(x):\n",
    "    import re\n",
    "    moves = re.split('\\d+\\. ', x)[1:]\n",
    "    return [x.strip() for x in moves]\n",
    "\n",
    "udf_movetype = F.udf(lambda x: movetype(x), T.ArrayType(T.StringType()))\n",
    "\n",
    "df = df.withColumn('moves', udf_movetype(F.col('AN')))\n",
    "df = df.drop('AN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event='Blitz', white='Nippis', black='Misha_44', result='1-0', UTCDate=datetime.date(2016, 1, 26), UTCTime='18:03:38', WhiteElo=2068, BlackElo=1846, WhiteRatingDiff=11.0, BlackRatingDiff=-5.0, ECO='A34', Opening='English Opening: Symmetrical Variation, Normal Variation', TimeControl='300+0', Termination='Normal', moves=['c4 c5', 'Nc3 Nf6', 'Nf3 g6', 'd4 cxd4', 'Nxd4 Bg7', 'g3 O-O', 'Bg2 Nc6', 'O-O Nxd4', 'Qxd4 d6', 'Qd2 Rb8', 'Rd1 Qa5', 'a3 a6', 'b4 Qd8', 'Bb2 Be6', 'Nd5 Nxd5', 'cxd5 Bd7', 'Rac1 Rc8', 'Bxg7 Kxg7', 'Qd4+ f6', 'Qa7 Rb8', 'Rc3 Rf7', 'Rdc1 Bb5', 'e4 h5', 'Bh3 h4', 'Be6 Rf8', 'Rc7 hxg3', 'hxg3 f5', 'Rxb7 Rxb7', 'Qxb7 fxe4', 'Rc7 Kf6', 'Rc8 e3', 'Rxd8 Rxd8', 'fxe3 Rh8', 'Qb6 Rh5', 'Qd4+ Re5', 'Qf4+ Rf5', 'Bxf5 gxf5', 'e4 Bd3', 'Qxf5+ Kg7', 'g4 Kg8', 'g5 Kg7', 'g6 a5', 'Qf7+ Kh6', 'g7 Bxe4', 'g8=Q Bxd5', 'Qfh7# 1-0'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- white: string (nullable = true)\n",
      " |-- black: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- UTCDate: date (nullable = true)\n",
      " |-- UTCTime: string (nullable = true)\n",
      " |-- WhiteElo: integer (nullable = true)\n",
      " |-- BlackElo: integer (nullable = true)\n",
      " |-- WhiteRatingDiff: double (nullable = true)\n",
      " |-- BlackRatingDiff: double (nullable = true)\n",
      " |-- ECO: string (nullable = true)\n",
      " |-- Opening: string (nullable = true)\n",
      " |-- TimeControl: string (nullable = true)\n",
      " |-- Termination: string (nullable = true)\n",
      " |-- moves: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is still broken.  SOmething's up with the job, but the df is correct from the above.\n",
    "df.printSchema()\n",
    "#df.write.parquet(\"../../data/processed/chess_games_moves.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event='Blitz', white='Nippis', black='Misha_44', result='1-0', UTCDate=datetime.date(2016, 1, 26), UTCTime='18:03:38', WhiteElo=2068, BlackElo=1846, WhiteRatingDiff=11.0, BlackRatingDiff=-5.0, ECO='A34', Opening='English Opening: Symmetrical Variation, Normal Variation', TimeControl='300+0', Termination='Normal', moves=['c4 c5', 'Nc3 Nf6', 'Nf3 g6', 'd4 cxd4', 'Nxd4 Bg7', 'g3 O-O', 'Bg2 Nc6', 'O-O Nxd4', 'Qxd4 d6', 'Qd2 Rb8', 'Rd1 Qa5', 'a3 a6', 'b4 Qd8', 'Bb2 Be6', 'Nd5 Nxd5', 'cxd5 Bd7', 'Rac1 Rc8', 'Bxg7 Kxg7', 'Qd4+ f6', 'Qa7 Rb8', 'Rc3 Rf7', 'Rdc1 Bb5', 'e4 h5', 'Bh3 h4', 'Be6 Rf8', 'Rc7 hxg3', 'hxg3 f5', 'Rxb7 Rxb7', 'Qxb7 fxe4', 'Rc7 Kf6', 'Rc8 e3', 'Rxd8 Rxd8', 'fxe3 Rh8', 'Qb6 Rh5', 'Qd4+ Re5', 'Qf4+ Rf5', 'Bxf5 gxf5', 'e4 Bd3', 'Qxf5+ Kg7', 'g4 Kg8', 'g5 Kg7', 'g6 a5', 'Qf7+ Kh6', 'g7 Bxe4', 'g8=Q Bxd5', 'Qfh7# 1-0'], first_two=['c4 c5', 'Nc3 Nf6']),\n",
       " Row(event='Blitz', white='abracadaver', black='andremoniy', result='1-0', UTCDate=datetime.date(2016, 1, 26), UTCTime='18:03:39', WhiteElo=1708, BlackElo=1399, WhiteRatingDiff=3.0, BlackRatingDiff=-3.0, ECO='A40', Opening='English Defense #2', TimeControl='180+0', Termination='Normal', moves=['d4 b6', 'c4 Bb7', 'Nc3 e6', 'a3 g6', 'Nf3 Bg7', 'g3 d5', 'cxd5 exd5', 'Bg2 Ne7', 'O-O Nf5', 'Re1 Na6', 'e4 dxe4', 'Nxe4 O-O', 'Be3 c5', 'Nc3 cxd4', 'Nxd4 Nxe3', 'fxe3 Bxg2', 'Kxg2 Qc7', 'Rc1 Qb7+', 'Qf3 Qc8', 'Nd5 Qd8', 'Nc6 Qg5', 'Rc2 Rac8', 'Nce7+ Kh8', 'Rxc8 Rxc8', 'Nxc8 Bxb2', 'Qxf7 Bxa3', 'Nce7 Bxe7', 'Nxe7 Qxe7', 'Qxe7 Kg8', 'Qxa7 Nc5', 'Rd1 Nd7', 'Rxd7 1-0'], first_two=['d4 b6', 'c4 Bb7'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first 2 moves and put into a new columns\n",
    "# https://stackoverflow.com/questions/52975567/get-first-n-elements-from-dataframe-arraytype-column-in-pyspark\n",
    "from pyspark.sql.functions import col, asc\n",
    "df=df.withColumn(\"first_two\", F.array([F.col(\"moves\")[0], F.col(\"moves\")[1]]))\n",
    "#df=df.withColumn(\"first_two_str\", F.array([F.col(\"moves\")[0], F.col(\"moves\")[1]]))\n",
    "\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|         first_two| count|\n",
      "+------------------+------+\n",
      "|  [e4 e5, Nf3 Nc6]|332485|\n",
      "|   [e4 e5, Nf3 d6]|129811|\n",
      "|[e4 d5, exd5 Qxd5]| 86593|\n",
      "|  [e4 c5, Nf3 Nc6]| 78661|\n",
      "|    [e4 e6, d4 d5]| 72138|\n",
      "+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None 3850385\n"
     ]
    }
   ],
   "source": [
    "print( df.groupBy('first_two').count().sort(col(\"count\").desc()).show(5), df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns are: 17\n"
     ]
    }
   ],
   "source": [
    "print (f\"Total columns are: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.filter(df.result.contains('*')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- white: string (nullable = true)\n",
      " |-- black: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- UTCDate: date (nullable = true)\n",
      " |-- UTCTime: string (nullable = true)\n",
      " |-- WhiteElo: integer (nullable = true)\n",
      " |-- BlackElo: integer (nullable = true)\n",
      " |-- WhiteRatingDiff: double (nullable = true)\n",
      " |-- BlackRatingDiff: double (nullable = true)\n",
      " |-- ECO: string (nullable = true)\n",
      " |-- Opening: string (nullable = true)\n",
      " |-- TimeControl: string (nullable = true)\n",
      " |-- Termination: string (nullable = true)\n",
      " |-- moves: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- first_two: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- result_label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode result\n",
    "# df=df.withColumn(\"enc_result\", F.array([F.col(\"result\")[0]]))\n",
    "# https://gist.github.com/zoltanctoth/2deccd69e3d1cde1dd78\n",
    "from pyspark.sql.types import IntegerType\n",
    "#label_udf  = F.udf(lambda result: 1 if result =='1-0' else 0, IntegerType())\n",
    "white_win_udf  = F.udf(lambda result:float(frac (result.split('-')[0])), IntegerType())\n",
    "black_win_udf  = F.udf(lambda result:float(frac (result.split('-')[1])), IntegerType())\n",
    "\n",
    "\n",
    "df=df.withColumn(\"whil\", label_udf(df.result))\n",
    "df=df.withColumn(\"result_label\", label_udf(df.result))\n",
    "\n",
    "df.take(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The input column first_two must be either string type or numeric type, but got ArrayType(StringType,true).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3c6c553459b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first_two\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first_two_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The input column first_two must be either string type or numeric type, but got ArrayType(StringType,true)."
     ]
    }
   ],
   "source": [
    "# Encode features-First_Two\n",
    "# https://spark.apache.org/docs/latest/ml-features#onehotencoder\n",
    "# https://spark.apache.org/docs/latest/ml-features#vectorindexer\n",
    "# https://datascience.stackexchange.com/questions/6268/how-to-convert-categorical-data-to-numerical-data-in-pyspark\n",
    "# https://silpara.medium.com/pyspark-string-to-array-of-string-in-dataframe-b9572233ccea\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"first_two\", outputCol=\"first_two_index\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables\n",
    "vars_to_keep = [\"median_house_value_final\", \n",
    "              \"total_bedrooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"median_income\", \n",
    "              \"rooms_per_household\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df=data.select(vars_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df_ex = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df_ex)\n",
    "encoded = model.transform(df_ex)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(data)\n",
    "\n",
    "categoricalFeatures = indexerModel.categoryMaps\n",
    "print(\"Chose %d categorical features: %s\" %\n",
    "      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(data)\n",
    "indexedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)\n",
    "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1]))).map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "df.select(\"result\").rdd.flatMap(lambda row:(1 if row[0]=='1-0' else 0)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest to determine which vairbales is the most influencial\n",
    "# *cartesian product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to determine influencial moves\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
